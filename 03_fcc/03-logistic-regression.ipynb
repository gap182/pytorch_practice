{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "from torchvision.datasets import MNIST \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvision.datasets es para importar diferentes tipos de data ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST('data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ojo, con len() puedo ver el tamaño del dataset, pero no es un tipo de dato común\n",
    "Es un tipo de dato especial del torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST('data/', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se escoge el train=False en el dataset del torchvision, entonces toma un conjunto\n",
    "de los datos destinados para el test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, quiero mirar como se ven los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=L size=28x28 at 0x7FCF9D3B13D0> 5\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0],dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que los elemenos del dataset son un tuble, con el primer elemento que\n",
    "es una imagen y el segundo el label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAN80lEQVR4nO3df6hcdXrH8c+ncf3DrBpTMYasNhuRWBWbLRqLSl2RrD9QNOqWDVgsBrN/GHChhEr6xyolEuqP0qAsuYu6sWyzLqgYZVkVo6ZFCF5j1JjU1YrdjV6SSozG+KtJnv5xT+Su3vnOzcyZOZP7vF9wmZnzzJnzcLife87Md879OiIEYPL7k6YbANAfhB1IgrADSRB2IAnCDiRxRD83ZpuP/oEeiwiPt7yrI7vtS22/aftt27d281oAesudjrPbniLpd5IWSNou6SVJiyJia2EdjuxAj/XiyD5f0tsR8U5EfCnpV5Ku6uL1APRQN2GfJekPYx5vr5b9EdtLbA/bHu5iWwC61M0HdOOdKnzjND0ihiQNSZzGA03q5si+XdJJYx5/R9L73bUDoFe6CftLkk61/V3bR0r6kaR19bQFoG4dn8ZHxD7bSyU9JWmKpAci4o3aOgNQq46H3jraGO/ZgZ7ryZdqABw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii4ymbcXiYMmVKsX7sscf2dPtLly5tWTvqqKOK686dO7dYv/nmm4v1u+66q2Vt0aJFxXU///zzYn3lypXF+u23316sN6GrsNt+V9IeSfsl7YuIs+toCkD96jiyXxQRH9TwOgB6iPfsQBLdhj0kPW37ZdtLxnuC7SW2h20Pd7ktAF3o9jT+/Ih43/YJkp6x/V8RsWHsEyJiSNKQJNmOLrcHoENdHdkj4v3qdqekxyTNr6MpAPXrOOy2p9o++uB9ST+QtKWuxgDUq5vT+BmSHrN98HX+PSJ+W0tXk8zJJ59crB955JHF+nnnnVesX3DBBS1r06ZNK6577bXXFutN2r59e7G+atWqYn3hwoUta3v27Cmu++qrrxbrL7zwQrE+iDoOe0S8I+kvauwFQA8x9AYkQdiBJAg7kARhB5Ig7EASjujfl9om6zfo5s2bV6yvX7++WO/1ZaaD6sCBA8X6jTfeWKx/8sknHW97ZGSkWP/www+L9TfffLPjbfdaRHi85RzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlrMH369GJ948aNxfqcOXPqbKdW7XrfvXt3sX7RRRe1rH355ZfFdbN+/6BbjLMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJM2VyDXbt2FevLli0r1q+44opi/ZVXXinW2/1L5ZLNmzcX6wsWLCjW9+7dW6yfccYZLWu33HJLcV3UiyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTB9ewD4JhjjinW200vvHr16pa1xYsXF9e9/vrri/W1a9cW6xg8HV/PbvsB2zttbxmzbLrtZ2y/Vd0eV2ezAOo3kdP4X0i69GvLbpX0bEScKunZ6jGAAdY27BGxQdLXvw96laQ11f01kq6uuS8ANev0u/EzImJEkiJixPYJrZ5oe4mkJR1uB0BNen4hTEQMSRqS+IAOaFKnQ287bM+UpOp2Z30tAeiFTsO+TtIN1f0bJD1eTzsAeqXtabzttZK+L+l429sl/VTSSkm/tr1Y0u8l/bCXTU52H3/8cVfrf/TRRx2ve9NNNxXrDz/8cLHebo51DI62YY+IRS1KF9fcC4Ae4uuyQBKEHUiCsANJEHYgCcIOJMElrpPA1KlTW9aeeOKJ4roXXnhhsX7ZZZcV608//XSxjv5jymYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9knulFNOKdY3bdpUrO/evbtYf+6554r14eHhlrX77ruvuG4/fzcnE8bZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtmTW7hwYbH+4IMPFutHH310x9tevnx5sf7QQw8V6yMjIx1vezJjnB1IjrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHUVnnnlmsX7PPfcU6xdf3Plkv6tXry7WV6xYUay/9957HW/7cNbxOLvtB2zvtL1lzLLbbL9ne3P1c3mdzQKo30RO438h6dJxlv9LRMyrfn5Tb1sA6tY27BGxQdKuPvQCoIe6+YBuqe3XqtP841o9yfYS28O2W/8zMgA912nYfybpFEnzJI1IurvVEyNiKCLOjoizO9wWgBp0FPaI2BER+yPigKSfS5pfb1sA6tZR2G3PHPNwoaQtrZ4LYDC0HWe3vVbS9yUdL2mHpJ9Wj+dJCknvSvpxRLS9uJhx9sln2rRpxfqVV17ZstbuWnl73OHir6xfv75YX7BgQbE+WbUaZz9iAisuGmfx/V13BKCv+LoskARhB5Ig7EAShB1IgrADSXCJKxrzxRdfFOtHHFEeLNq3b1+xfskll7SsPf/888V1D2f8K2kgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLtVW/I7ayzzirWr7vuumL9nHPOaVlrN47eztatW4v1DRs2dPX6kw1HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2SW7u3LnF+tKlS4v1a665plg/8cQTD7mnidq/f3+xPjJS/u/lBw4cqLOdwx5HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2w0C7sexFi8abaHdUu3H02bNnd9JSLYaHh4v1FStWFOvr1q2rs51Jr+2R3fZJtp+zvc32G7ZvqZZPt/2M7beq2+N63y6ATk3kNH6fpL+PiD+X9FeSbrZ9uqRbJT0bEadKerZ6DGBAtQ17RIxExKbq/h5J2yTNknSVpDXV09ZIurpXTQLo3iG9Z7c9W9L3JG2UNCMiRqTRPwi2T2ixzhJJS7prE0C3Jhx229+W9Iikn0TEx/a4c8d9Q0QMSRqqXoOJHYGGTGjozfa3NBr0X0bEo9XiHbZnVvWZknb2pkUAdWh7ZPfoIfx+Sdsi4p4xpXWSbpC0srp9vCcdTgIzZswo1k8//fRi/d577y3WTzvttEPuqS4bN24s1u+8886WtccfL//KcIlqvSZyGn++pL+V9LrtzdWy5RoN+a9tL5b0e0k/7E2LAOrQNuwR8Z+SWr1Bv7jedgD0Cl+XBZIg7EAShB1IgrADSRB2IAkucZ2g6dOnt6ytXr26uO68efOK9Tlz5nTUUx1efPHFYv3uu+8u1p966qli/bPPPjvkntAbHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIk04+znnntusb5s2bJiff78+S1rs2bN6qinunz66acta6tWrSque8cddxTre/fu7agnDB6O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRJpx9oULF3ZV78bWrVuL9SeffLJY37dvX7FeuuZ89+7dxXWRB0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUjCEVF+gn2SpIcknSjpgKShiPhX27dJuknS/1ZPXR4Rv2nzWuWNAehaRIw76/JEwj5T0syI2GT7aEkvS7pa0t9I+iQi7ppoE4Qd6L1WYZ/I/Owjkkaq+3tsb5PU7L9mAXDIDuk9u+3Zkr4naWO1aKnt12w/YPu4FusssT1se7irTgF0pe1p/FdPtL8t6QVJKyLiUdszJH0gKST9k0ZP9W9s8xqcxgM91vF7dkmy/S1JT0p6KiLuGac+W9KTEXFmm9ch7ECPtQp729N425Z0v6RtY4NefXB30EJJW7ptEkDvTOTT+Ask/Yek1zU69CZJyyUtkjRPo6fx70r6cfVhXum1OLIDPdbVaXxdCDvQex2fxgOYHAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9HvK5g8k/c+Yx8dXywbRoPY2qH1J9NapOnv7s1aFvl7P/o2N28MRcXZjDRQMam+D2pdEb53qV2+cxgNJEHYgiabDPtTw9ksGtbdB7Uuit071pbdG37MD6J+mj+wA+oSwA0k0Enbbl9p+0/bbtm9toodWbL9r+3Xbm5uen66aQ2+n7S1jlk23/Yztt6rbcefYa6i322y/V+27zbYvb6i3k2w/Z3ub7Tds31Itb3TfFfrqy37r+3t221Mk/U7SAknbJb0kaVFEbO1rIy3YflfS2RHR+BcwbP+1pE8kPXRwai3b/yxpV0SsrP5QHhcR/zAgvd2mQ5zGu0e9tZpm/O/U4L6rc/rzTjRxZJ8v6e2IeCcivpT0K0lXNdDHwIuIDZJ2fW3xVZLWVPfXaPSXpe9a9DYQImIkIjZV9/dIOjjNeKP7rtBXXzQR9lmS/jDm8XYN1nzvIelp2y/bXtJ0M+OYcXCarer2hIb7+bq203j309emGR+YfdfJ9OfdaiLs401NM0jjf+dHxF9KukzSzdXpKibmZ5JO0egcgCOS7m6ymWqa8Uck/SQiPm6yl7HG6asv+62JsG+XdNKYx9+R9H4DfYwrIt6vbndKekyjbzsGyY6DM+hWtzsb7ucrEbEjIvZHxAFJP1eD+66aZvwRSb+MiEerxY3vu/H66td+ayLsL0k61fZ3bR8p6UeS1jXQxzfYnlp9cCLbUyX9QIM3FfU6STdU92+Q9HiDvfyRQZnGu9U042p43zU+/XlE9P1H0uUa/UT+vyX9YxM9tOhrjqRXq583mu5N0lqNntb9n0bPiBZL+lNJz0p6q7qdPkC9/ZtGp/Z+TaPBmtlQbxdo9K3ha5I2Vz+XN73vCn31Zb/xdVkgCb5BByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/D+f1mbtgJ8kQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('label: ', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces como el tipo de imagen es PIL, se usa imshow para graficar los datos,\n",
    "además, se puede ver que la imagen es de tamaño 28x28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ojo, al parecer no puedo graficar con una iteración, porque debo pasar el dataset\n",
    "por un TensorData\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OJO, no olvidar que estos datos no son aún tensores, sin simplemente imágenes, se deben\n",
    "convertir a tensores, para eso al momento de importar debo escoger una opción llamada \n",
    "**transform** y debe ser igual a **transforms.ToTensor()**\n",
    "\n",
    "**OJO** Es necesario importar el transforms de el torchvision.\n",
    "\n",
    "Viendo la documentación de transform, hay muchas opciones para operar sobre imágenes, se puede\n",
    "también importar como tensor pero hacer operaciones antes como normalización, transformación lineal, borrado random.\n",
    "\n",
    "También se puede usar transform para el proceso inverso, convertir de tensor a imagen (debo\n",
    "revisarlo luego).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, nuevamente el dataset está dividido en la imagen y el laben (dato entrada y salida) \n",
    "pero esta ves no se puede graficar al ser un tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'int'>\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST('data/', download=True, transform=transforms.ToTensor())\n",
    "\n",
    "image_tensor, label = dataset[0]\n",
    "# print(image,label)\n",
    "print(type(image_tensor), type(label))\n",
    "print(image_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al parecer aunque ahora sea un tensor, lo puedo graficar, pensaba que no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcf9c672c10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_tensor[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora voy a dividir los datos en uno de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(n, val_pct):\n",
    "    n_val = int(n*val_pct)\n",
    "    indxs = np.random.permutation(n)\n",
    "    #ojo, no me servía el randint porque podría repetir los índices\n",
    "    return indxs[n_val:], indxs[:n_val]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = split_indices(len(dataset),val_pct = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_indices.shape, val_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OJO, el sampler es una forma de organizar los items dentro del dataloader, hay varias opciones, sería bueno revisarlas en algún momento a detalle. Por ahora, el subsetrandomsampler toma una lista, o array de indices y genera un sampler de esos índices de manera aleatorioa.\n",
    "\n",
    "Ahora, el dataloader es para organizar los datos de manera iterable en datos de entrada y salida, con batches también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definí simplemente un modelo lineal, el problema es que para la clasificación de los número vamos a usar onehot representation, por lo que toca hacer un softmax y un crossentropy para el error.\n",
    "\n",
    "OJO, los parámetros definidos dentro del módulo nn son automáticamente requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0315, -0.0142,  0.0044,  ...,  0.0095,  0.0146, -0.0131],\n",
      "        [ 0.0347,  0.0195, -0.0312,  ...,  0.0100, -0.0014, -0.0304],\n",
      "        [ 0.0004, -0.0210,  0.0247,  ..., -0.0247, -0.0264,  0.0044],\n",
      "        ...,\n",
      "        [ 0.0224,  0.0077,  0.0204,  ...,  0.0034,  0.0219,  0.0077],\n",
      "        [ 0.0220, -0.0245, -0.0297,  ...,  0.0345, -0.0158, -0.0055],\n",
      "        [ 0.0274,  0.0217,  0.0026,  ..., -0.0042,  0.0088,  0.0148]],\n",
      "       requires_grad=True) Parameter containing:\n",
      "tensor([ 0.0226,  0.0241, -0.0077,  0.0250,  0.0058, -0.0245, -0.0101,  0.0215,\n",
      "        -0.0137,  0.0043], requires_grad=True)\n",
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.weight, model.bias); print(model.weight.shape, model.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 1, 0, 1, 9, 3, 1, 0, 4, 1, 1, 2, 1, 7, 9, 8, 7, 1, 7, 1, 2, 5, 8, 0,\n",
      "        1, 8, 6, 4, 9, 4, 6, 7, 3, 6, 2, 7, 8, 4, 4, 7, 9, 5, 1, 0, 4, 0, 3, 2,\n",
      "        6, 1, 1, 5, 8, 2, 6, 4, 0, 7, 3, 2, 8, 4, 8, 5, 9, 3, 9, 2, 0, 7, 6, 0,\n",
      "        4, 6, 6, 2, 4, 7, 1, 1, 4, 6, 2, 2, 5, 1, 4, 9, 8, 2, 1, 2, 8, 8, 5, 0,\n",
      "        0, 8, 2, 9])\n",
      "torch.Size([100, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [2800 x 28], m2: [784 x 10] at /opt/conda/conda-bld/pytorch_1587428398394/work/aten/src/TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-64ad1e558d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [2800 x 28], m2: [784 x 10] at /opt/conda/conda-bld/pytorch_1587428398394/work/aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs = model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OJO** El error anterior se debe a que los datos de entrada no tienen la dimensión correcta, deben aplanarse a 28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos entonces hacer la conversión de tamaño antes de, o la podemos definir dentro de nuestro modelo mismo, en el forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTmodel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MNISTmodel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1,784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "model = MNISTmodel(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 1, 0, 1, 9, 3, 1, 0, 4, 1, 1, 2, 1, 7, 9, 8, 7, 1, 7, 1, 2, 5, 8, 0,\n",
      "        1, 8, 6, 4, 9, 4, 6, 7, 3, 6, 2, 7, 8, 4, 4, 7, 9, 5, 1, 0, 4, 0, 3, 2,\n",
      "        6, 1, 1, 5, 8, 2, 6, 4, 0, 7, 3, 2, 8, 4, 8, 5, 9, 3, 9, 2, 0, 7, 6, 0,\n",
      "        4, 6, 6, 2, 4, 7, 1, 1, 4, 6, 2, 2, 5, 1, 4, 9, 8, 2, 1, 2, 8, 8, 5, 0,\n",
      "        0, 8, 2, 9])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "for images, label in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    print(outputs.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n",
      "<generator object Module.parameters at 0x7fcf9cd04cd0>\n"
     ]
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "print(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10]) tensor([[-0.2342,  0.2149, -0.2024,  0.2304,  0.2194, -0.2268, -0.1529, -0.1125,\n",
      "          0.3382, -0.1060],\n",
      "        [-0.3769,  0.3660,  0.0084,  0.4478,  0.3935, -0.1509, -0.2171,  0.1230,\n",
      "          0.1677, -0.0896]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "    \n",
    "print(outputs.shape, outputs[:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_out = F.softmax(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prob, pred = torch.max(prob_out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 3, 3, 4, 8, 4, 8, 3, 3, 3, 9, 1, 3, 3, 1, 8, 6, 3, 1, 8, 8, 8, 3, 8,\n",
      "        8, 1, 8, 3, 3, 3, 3, 4, 8, 1, 1, 3, 4, 8, 8, 3, 3, 8, 3, 3, 4, 3, 3, 8,\n",
      "        3, 8, 8, 1, 8, 3, 3, 3, 8, 3, 1, 8, 8, 8, 8, 1, 4, 8, 3, 3, 8, 1, 3, 8,\n",
      "        8, 3, 8, 4, 3, 1, 4, 1, 3, 8, 8, 4, 3, 3, 3, 8, 3, 3, 4, 3, 3, 7, 1, 4,\n",
      "        8, 3, 3, 4])\n",
      "tensor([9, 4, 5, 2, 4, 7, 7, 8, 1, 4, 6, 4, 1, 6, 9, 1, 6, 2, 7, 7, 3, 1, 3, 9,\n",
      "        0, 7, 1, 0, 6, 4, 0, 4, 3, 5, 9, 6, 8, 8, 1, 3, 9, 1, 7, 2, 2, 1, 1, 8,\n",
      "        6, 9, 5, 9, 8, 8, 3, 6, 2, 6, 5, 9, 8, 4, 3, 5, 2, 9, 1, 6, 9, 7, 6, 9,\n",
      "        7, 6, 5, 1, 3, 9, 3, 8, 0, 1, 5, 8, 1, 0, 1, 8, 2, 8, 6, 3, 3, 3, 1, 9,\n",
      "        3, 2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(l1,l2):\n",
    "    return torch.sum(l1==l2).item()/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función accuracy es buena para medir el modelo, pero no para optimizarlo, ya que no es diferenciable. Para eso podemos usar cross entroy, el cual usa la siguiente ecuación:\n",
    "\n",
    "$$ D(\\hat{y},y) = -\\sum_j{y_j ln \\hat{y_j}} $$\n",
    "\n",
    "Donde $\\hat{y}$ es la probabilidad predicha y $ y $ es la probabilidad real del label.\n",
    "El signo negativo es debido a que la probabilidad predicha va de 0 a 1, por lo que el ln es negativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay una función en pytorch que primero hace el softmax y luego el cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2973, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(outputs,labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debo definir el optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "opt = torch.optim.SGD(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None, metric=None):\n",
    "    pred = model(xb)\n",
    "    loss = loss_func(pred, yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "        metric_result = metric(pred,yb)\n",
    "    \n",
    "    return loss.item(), len(xb), metric_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OJO** el optimizador y la métrica son opcionales en esta función, para asegurarme que la función la podré utilizar para calcular el loss para los datos de validación y testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_func, valid_dl, metric=None):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        results = [loss_batch(model, loss_func, xb, yb, metric=metric) for xb, yb in valid_dl]\n",
    "\n",
    "        losses, nums, metrics = zip(*results)\n",
    "\n",
    "        total = np.sum(nums)\n",
    "\n",
    "        avg_loss = np.sum(np.multiply(losses,nums))/total\n",
    "\n",
    "        avg_metric = None\n",
    "\n",
    "        if metric is not None:\n",
    "            avg_metric = np.sum(np.multiply(metrics,nums))/total\n",
    "\n",
    "    return avg_loss, total, avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.sum(preds==labels).item()/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, total, val_acc = evaluate(model,loss_fn,val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3043393035729727 12000 0.12466666666666666\n"
     ]
    }
   ],
   "source": [
    "print(val_loss, total, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric=None):\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            loss,_,_ = loss_batch(model, loss_fn, xb, yb, opt)\n",
    "\n",
    "        result = evaluate(model,loss_fn, valid_dl,metric=metric)\n",
    "        val_loss, total, val_metric = result\n",
    "\n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, val_loss))\n",
    "\n",
    "        else:\n",
    "            print('Epoch [{}/{}], Loss: {}, {}: {:.4f}'.format(epoch+1,epochs, loss,metric.__name__, val_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTmodel(input_size, num_classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.43021854758262634, accuracy: 0.8719\n",
      "Epoch [2/5], Loss: 0.5321688652038574, accuracy: 0.8723\n",
      "Epoch [3/5], Loss: 0.5593116283416748, accuracy: 0.8731\n",
      "Epoch [4/5], Loss: 0.5725670456886292, accuracy: 0.8736\n",
      "Epoch [5/5], Loss: 0.5799338817596436, accuracy: 0.8745\n"
     ]
    }
   ],
   "source": [
    "fit(5, model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.48510056734085083, accuracy: 0.8749\n",
      "Epoch [2/5], Loss: 0.49651899933815, accuracy: 0.8753\n",
      "Epoch [3/5], Loss: 0.5016648769378662, accuracy: 0.8758\n",
      "Epoch [4/5], Loss: 0.3379616141319275, accuracy: 0.8762\n",
      "Epoch [5/5], Loss: 0.4077931344509125, accuracy: 0.8771\n"
     ]
    }
   ],
   "source": [
    "fit(5, model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.3813748061656952, accuracy: 0.8776\n",
      "Epoch [2/5], Loss: 0.412529855966568, accuracy: 0.8782\n",
      "Epoch [3/5], Loss: 0.4845489263534546, accuracy: 0.8789\n",
      "Epoch [4/5], Loss: 0.40327563881874084, accuracy: 0.8794\n",
      "Epoch [5/5], Loss: 0.4235337972640991, accuracy: 0.8800\n"
     ]
    }
   ],
   "source": [
    "fit(5, model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.4611455798149109, accuracy: 0.8808\n",
      "Epoch [2/5], Loss: 0.4005926549434662, accuracy: 0.8813\n",
      "Epoch [3/5], Loss: 0.5786811113357544, accuracy: 0.8815\n",
      "Epoch [4/5], Loss: 0.5154942870140076, accuracy: 0.8820\n",
      "Epoch [5/5], Loss: 0.45869046449661255, accuracy: 0.8826\n"
     ]
    }
   ],
   "source": [
    "fit(5, model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf9f78d710>]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = [0.8719,0.8723,0.8731,0.8736,0.8745,0.8749,0.8753,0.8758,0.8762,0.8771,\n",
    "             0.8776,0.8782,0.8789,0.8794,0.8800,0.8808,0.8813,0.8815,0.8820,0.8826]\n",
    "\n",
    "plt.plot(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas varias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [loss_batch(model, loss_fn, xb, yb, metric=accuracy) for xb, yb in val_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.342181921005249 100 0.13\n",
      "[(2.342181921005249, 100, 0.13), (2.318309783935547, 100, 0.21), (2.3299460411071777, 100, 0.1), (2.350898504257202, 100, 0.09), (2.3308675289154053, 100, 0.09), (2.338451385498047, 100, 0.13), (2.295437812805176, 100, 0.18), (2.3414549827575684, 100, 0.1), (2.3385541439056396, 100, 0.12), (2.3302321434020996, 100, 0.12), (2.340193510055542, 100, 0.13), (2.3320140838623047, 100, 0.17), (2.321310043334961, 100, 0.1), (2.3518314361572266, 100, 0.1), (2.3188295364379883, 100, 0.08), (2.3416831493377686, 100, 0.16), (2.2768173217773438, 100, 0.17), (2.328573703765869, 100, 0.14), (2.3249237537384033, 100, 0.15), (2.356862783432007, 100, 0.08), (2.3471524715423584, 100, 0.1), (2.3517768383026123, 100, 0.08), (2.3567960262298584, 100, 0.09), (2.298767328262329, 100, 0.15), (2.293274164199829, 100, 0.14), (2.3206210136413574, 100, 0.14), (2.3376495838165283, 100, 0.05), (2.3733458518981934, 100, 0.06), (2.3303637504577637, 100, 0.12), (2.3201565742492676, 100, 0.16), (2.3216049671173096, 100, 0.11), (2.3365352153778076, 100, 0.11), (2.344024896621704, 100, 0.1), (2.3412795066833496, 100, 0.1), (2.320657730102539, 100, 0.18), (2.347557306289673, 100, 0.13), (2.3187172412872314, 100, 0.13), (2.379389762878418, 100, 0.1), (2.3647522926330566, 100, 0.07), (2.344578742980957, 100, 0.06), (2.34566068649292, 100, 0.1), (2.3000717163085938, 100, 0.18), (2.320138931274414, 100, 0.12), (2.306605100631714, 100, 0.14), (2.364211082458496, 100, 0.05), (2.296492099761963, 100, 0.14), (2.288139581680298, 100, 0.15), (2.313988208770752, 100, 0.12), (2.302842617034912, 100, 0.09), (2.3537566661834717, 100, 0.07), (2.347447395324707, 100, 0.07), (2.296339273452759, 100, 0.13), (2.347393035888672, 100, 0.07), (2.3348209857940674, 100, 0.17), (2.3103160858154297, 100, 0.11), (2.3151116371154785, 100, 0.14), (2.3284342288970947, 100, 0.16), (2.3288776874542236, 100, 0.12), (2.3219072818756104, 100, 0.11), (2.3360698223114014, 100, 0.1), (2.3542912006378174, 100, 0.11), (2.3565053939819336, 100, 0.08), (2.302673101425171, 100, 0.13), (2.3540830612182617, 100, 0.14), (2.3107850551605225, 100, 0.08), (2.372089147567749, 100, 0.06), (2.335191011428833, 100, 0.08), (2.3240442276000977, 100, 0.14), (2.2985341548919678, 100, 0.13), (2.3383758068084717, 100, 0.11), (2.316272020339966, 100, 0.14), (2.351836919784546, 100, 0.15), (2.3453800678253174, 100, 0.11), (2.3155529499053955, 100, 0.14), (2.3326215744018555, 100, 0.09), (2.3256380558013916, 100, 0.1), (2.3463733196258545, 100, 0.12), (2.3346059322357178, 100, 0.11), (2.3195505142211914, 100, 0.12), (2.280625104904175, 100, 0.18), (2.3350143432617188, 100, 0.13), (2.3390071392059326, 100, 0.08), (2.308029890060425, 100, 0.12), (2.31376576423645, 100, 0.1), (2.315694808959961, 100, 0.12), (2.345308542251587, 100, 0.12), (2.348336696624756, 100, 0.12), (2.3481457233428955, 100, 0.14), (2.324381113052368, 100, 0.09), (2.3383679389953613, 100, 0.1), (2.3074827194213867, 100, 0.1), (2.3263051509857178, 100, 0.13), (2.303689956665039, 100, 0.18), (2.362694501876831, 100, 0.1), (2.328386068344116, 100, 0.1), (2.3150570392608643, 100, 0.11), (2.2893829345703125, 100, 0.15), (2.321518659591675, 100, 0.11), (2.3722126483917236, 100, 0.14), (2.372351884841919, 100, 0.13), (2.359092950820923, 100, 0.08), (2.3396778106689453, 100, 0.11), (2.3566112518310547, 100, 0.16), (2.3231382369995117, 100, 0.07), (2.342729330062866, 100, 0.08), (2.322960138320923, 100, 0.14), (2.3283286094665527, 100, 0.1), (2.3562471866607666, 100, 0.1), (2.3547184467315674, 100, 0.1), (2.3313112258911133, 100, 0.16), (2.326787233352661, 100, 0.13), (2.324575185775757, 100, 0.11), (2.308997392654419, 100, 0.09), (2.36274790763855, 100, 0.12), (2.307851791381836, 100, 0.13), (2.304521083831787, 100, 0.13), (2.3760128021240234, 100, 0.1), (2.3957712650299072, 100, 0.07), (2.309175491333008, 100, 0.13), (2.3113203048706055, 100, 0.16)]\n"
     ]
    }
   ],
   "source": [
    "print(a[0],b[0], c[0])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2, 4), (5, 6, 7), (8, 9, 10)] (1, 2, 4) (5, 6, 7) (8, 9, 10)\n"
     ]
    }
   ],
   "source": [
    "# print(*results)\n",
    "lista_prueba = [(1,2,4),(5,6,7),(8,9,10)]\n",
    "print(lista_prueba, *lista_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ver = zip(*lista_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = a_ver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
